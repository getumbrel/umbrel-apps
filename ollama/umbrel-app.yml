manifestVersion: 1
id: ollama
name: Ollama
tagline: Get up and running with large language models
category: ai
version: "0.5.7"
port: 11434
description: >-
  ðŸ¦™ Ollama is a self-hosted service that allows you to download and run large language models on your own hardware.


  To connect to ollama, use the following URLs/IPs: 
    - From other Umbrel Apps
      - ollama_ollama_1:11434
      - 10.21.0.1:11434
    - From your local network
      - umbrel.local:11434
      - YOUR-IP:11434

  You can use clients like ðŸ’¬ **Open WebUI** or ðŸ¤¯ **Lobe Chat** which are user-friendly interfaces for managing and chatting with your AI models.

  A list of models can be found at https://ollama.com/search.


  You can also use the cli to manage ollama from the command line.
  Run the following commands to use the umbrel ollama instead of the local one:
    - export OLLAMA_HOST=http://umbrel.local:11434
    - ollama run llama3.2:1b
developer: Ollama
website: https://ollama.com/
submitter: al-lac
submission: https://github.com/getumbrel/umbrel-apps/pull/2114
repo: https://github.com/ollama/ollama
support: https://github.com/ollama/ollama/issues
gallery: []
defaultUsername: ""
defaultPassword: ""
dependencies: []
releaseNotes: ""
path: ""
